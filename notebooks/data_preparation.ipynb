{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db567772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import datetime, sys, sklearn\n",
    "\n",
    "ROOT_DIR = Path(__file__).resolve().parents[1] if \"__file__\" in locals() else Path.cwd().parents[0]\n",
    "RAW_PATH = ROOT_DIR / \"data\" / \"raw\" / \"IBMTelco_Datensatz.csv\"\n",
    "OUT_DIR = ROOT_DIR / \"data\" / \"processed\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1364cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RAW_PATH)\n",
    "df = df.rename(columns=lambda c: c.strip())\n",
    "df = df.drop(columns=[\"customerID\"])\n",
    "df[\"Churn\"] = df[\"Churn\"].map({\"Yes\":1, \"No\":0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d2d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=random_state)\n",
    "train_val_idx, test_idx = next(sss1.split(df, y))\n",
    "train_val = df.iloc[train_val_idx].reset_index(drop=True)\n",
    "test = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.1764705882, random_state=random_state)\n",
    "train_idx, val_idx = next(sss2.split(train_val, train_val[\"Churn\"]))\n",
    "train = train_val.iloc[train_idx].reset_index(drop=True)\n",
    "val = train_val.iloc[val_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5cf24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [train, val, test]\n",
    "for d in df_list:\n",
    "    d[\"TotalCharges\"] = d[\"TotalCharges\"].replace(\" \", np.nan).astype(float)\n",
    "    mask = d[\"tenure\"] == 0\n",
    "    d.loc[mask, \"TotalCharges\"] = d.loc[mask, \"TotalCharges\"].fillna(0)\n",
    "    d[\"SeniorCitizen\"] = d[\"SeniorCitizen\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d628fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal mappings\n",
    "mapping_contract = {\"Month-to-month\":1, \"One year\":12, \"Two year\":24}\n",
    "mapping_internet = {\"No\":0, \"DSL\":1, \"Fiber optic\":2}\n",
    "for d in df_list:\n",
    "    d[\"Contract\"] = d[\"Contract\"].map(mapping_contract)\n",
    "    d[\"InternetService\"] = d[\"InternetService\"].map(mapping_internet)\n",
    "\n",
    "# One-Hot\n",
    "onehot_cols = [\n",
    "    'gender','Partner','Dependents','PhoneService','MultipleLines',\n",
    "    'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport',\n",
    "    'StreamingTV','StreamingMovies','PaperlessBilling','PaymentMethod'\n",
    "]\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=\"first\", handle_unknown=\"ignore\")\n",
    "\n",
    "train_encoded = pd.DataFrame(encoder.fit_transform(train[onehot_cols]),\n",
    "                             columns=encoder.get_feature_names_out(onehot_cols), index=train.index)\n",
    "val_encoded = pd.DataFrame(encoder.transform(val[onehot_cols]),\n",
    "                           columns=encoder.get_feature_names_out(onehot_cols), index=val.index)\n",
    "test_encoded = pd.DataFrame(encoder.transform(test[onehot_cols]),\n",
    "                            columns=encoder.get_feature_names_out(onehot_cols), index=test.index)\n",
    "\n",
    "train = pd.concat([train.drop(columns=onehot_cols), train_encoded], axis=1)\n",
    "val = pd.concat([val.drop(columns=onehot_cols), val_encoded], axis=1)\n",
    "test = pd.concat([test.drop(columns=onehot_cols), test_encoded], axis=1)\n",
    "\n",
    "for d in [train,val,test]:\n",
    "    d.columns = d.columns.str.replace(\" \", \"_\").str.replace(\"-\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88517986",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "num_cols = [\"tenure\",\"MonthlyCharges\",\"TotalCharges\"]\n",
    "\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "val[num_cols] = scaler.transform(val[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ce00af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({0: 3621, 1: 1308})\n",
      "After SMOTE: Counter({0: 3621, 1: 3621})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "X_train = train.drop(columns=[\"Churn\"])\n",
    "y_train = train[\"Churn\"]\n",
    "\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "smote = SMOTE(random_state=random_state)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE:\", Counter(y_train_res))\n",
    "\n",
    "train_bal = pd.concat([X_train_res, y_train_res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3a68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bal.to_csv(OUT_DIR / \"train.csv\", index=False)\n",
    "val.to_csv(OUT_DIR / \"val.csv\", index=False)\n",
    "test.to_csv(OUT_DIR / \"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa67ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = sorted(set(train.columns) | set(val.columns) | set(test.columns))\n",
    "\n",
    "for name, df in zip([\"train\", \"val\", \"test\"], [train, val, test]):\n",
    "    missing_cols = set(all_cols) - set(df.columns)\n",
    "    for col in missing_cols:\n",
    "        df[col] = 0\n",
    "    df = df[all_cols]\n",
    "    df.to_csv(ROOT_DIR / \"data\" / \"processed\" / f\"{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cf40e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1118"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "md = f\"\"\"# Data Preparation Summary\n",
    "\n",
    "**Datum/Zeit:** {now}  \n",
    "**random_state:** {random_state}  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "All preprocessing steps (Data Selection, Cleaning, Encoding, Scaling, SMOTE) were executed in one reproducible pipeline.  \n",
    "Intermediate results were stored in `/data/processed/`.\n",
    "\n",
    "---\n",
    "\n",
    "## Processed Files\n",
    "- data/processed/train.csv  \n",
    "- data/processed/val.csv  \n",
    "- data/processed/test.csv  \n",
    "\n",
    "---\n",
    "\n",
    "## Transformation Overview\n",
    "| Phase | Transformation | Tools |\n",
    "|--------|----------------|--------|\n",
    "| 1 | Stratified Split (70/15/15) | scikit-learn StratifiedShuffleSplit |\n",
    "| 2 | Cleaning (fix TotalCharges, types) | pandas |\n",
    "| 3 | Encoding (ordinal + one-hot) | sklearn.preprocessing.OneHotEncoder |\n",
    "| 4 | Scaling (0â€“1) | sklearn.preprocessing.MinMaxScaler |\n",
    "| 5 | Balancing (SMOTE) | imblearn.over_sampling.SMOTE |\n",
    "\n",
    "---\n",
    "\n",
    "## Reproducibility\n",
    "- Python {sys.version.split()[0]}  \n",
    "- pandas {pd.__version__}  \n",
    "- scikit-learn {sklearn.__version__}\n",
    "\n",
    "---\n",
    "\n",
    "## Note\n",
    "This notebook ensures full reproducibility of the Data Preparation phase according to **CRISP-ML(Q)** standards.  \n",
    "All subsequent modeling and deployment steps should use these processed datasets as input.\n",
    "\"\"\"\n",
    "\n",
    "REPORT_DIR = ROOT_DIR / \"reports\" / \"data_preparation\"\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "Path(REPORT_DIR / \"data_preparation_summary.md\").write_text(md, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
